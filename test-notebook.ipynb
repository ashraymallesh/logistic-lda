{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "\n",
    "# Some helper functions:\n",
    "def sigmoid(z):\n",
    "    #return 1 / (1 + np.exp(-z))\n",
    "    return expit(z)\n",
    "\n",
    "def label(v):\n",
    "    \"\"\" \n",
    "    v is a vector (numpy ndarray)\n",
    "    this function updates each row to 1 if >= 0.5 and 0 if <0.5\n",
    "    \"\"\"\n",
    "    v[v>=0.5] = 1\n",
    "    v[v<0.5] = 0\n",
    "    return v\n",
    "\n",
    "#Data Cleaning\n",
    "winedf = pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "wineData = winedf.to_numpy()\n",
    "wineData[:,-1] = (wineData[:,-1]>=6).astype(int) #convert 6+ to 1 and <5 to 0\n",
    "\n",
    "bcdf = pd.read_csv(\"breast-cancer-wisconsin.data\", sep=',', header=None)\n",
    "bcdf.columns = ['Sample code', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                'Normal Nucleoli', 'Mitoses', 'Class']\n",
    "bcdf.replace('?', np.NaN, inplace=True)\n",
    "bcdf.dropna(inplace=True)\n",
    "bcdf.drop(['Sample code'], axis=1, inplace=True)\n",
    "bcData = bcdf.to_numpy().astype(float)\n",
    "bcData[:,-1] = (bcData[:,-1]>3).astype(int) #change 2/4 last column to 0/1 labels\n",
    "\n",
    "def train_test_split(dataset, ratio=0.2):\n",
    "    \"\"\"\n",
    "    split dataset into training and test subsets\n",
    "    test set size will be ratio * dataset size (default = 20% of total size)\n",
    "    returns data_train, data_test\n",
    "    \"\"\"\n",
    "    dataset_size = dataset.shape[0]\n",
    "    test_set_size = math.floor(ratio * dataset_size)\n",
    "    training_set_size = dataset_size - test_set_size\n",
    "    data_train = dataset[0:training_set_size, :]\n",
    "    data_test = dataset[0:test_set_size, :]\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"Implementing a Logistic Regression model without sklearn\"\"\"\n",
    "\n",
    "    def __init__(self, data, alpha=0.001):\n",
    "        \"\"\"\n",
    "\n",
    "        Constructor to create a new LogisticRegression instance\n",
    "\n",
    "        data        = matrix (numpy ndarray) of dataset with labels as the last column\n",
    "        X           = matrix (numpy ndarray) of dataset with labels (last column) removed\n",
    "        y           = vector (numpy ndarray) of labels (last column of data)\n",
    "        m           = number of training examples (= number of rows of X)\n",
    "        n           = number of features (= number of columns of X)\n",
    "        weights     = vector (numpy ndarray) of weights for gradient descent\n",
    "        alpha       = learning rate that controls how quickly gradient descent will converge\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.X = data[:,:-1]\n",
    "        self.m = self.X.shape[0]\n",
    "        self.y = self.data[:,-1][:, np.newaxis]\n",
    "        X = np.c_[np.ones(shape=(self.m,1)), self.X] #add bias column of ones to feature matrix\n",
    "        self.n = self.X.shape[1]\n",
    "        self.weights = np.zeros(shape=(self.n, 1)) #set initial weights vector to n-dim vector of zeros\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    # Model implementation begins below:     \n",
    "    def fit(self, steps):\n",
    "        \"\"\"\n",
    "        Fit training data using gradient descent (GD update perfomed 'steps' number of times)\n",
    "        Calculates and updates optimal weights for the model after \"training\" with data.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(steps):\n",
    "            self.weights = self.weights + ( self.alpha * (self.X.T @ (self.y - sigmoid(self.X @ self.weights))) )\n",
    "            # X @ weights ==> matrix multiplication of mxn and nx1 produces a mx1 vector\n",
    "            # then multiplying by X.T is a nxm @ mx1 produces a nx1 vector\n",
    "            # this numpy vectorized implementation of the gradient descent update is a lot more efficient than\n",
    "            # manually updating each weight using a python for-loop to calculate summmations\n",
    "        #return self.weights\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Given a trained model, predict labels for new data X_test (which is a mxn matrix),\n",
    "        mxn @ nx1 gives a mx1 vector of predicted 0/1 labels. Sigmoid function calculates\n",
    "        a vector of probabilities where each row is the probablity of being classified positive (1)\n",
    "\n",
    "        This vector is passed into a \"label\" function which outputs 1 if probability>=0.5\n",
    "        and 0 if probability<0.5\n",
    "\n",
    "        predict returns a m-dimensional vector of 0/1 predicted labels\n",
    "        \"\"\"\n",
    "        predicted_labels = label( sigmoid(X_test @ self.weights) )\n",
    "        return predicted_labels\n",
    "\n",
    "    def evaluate_acc(self, y_predicted, y_test):\n",
    "        \"\"\"\n",
    "        Check the accuracy of the predictions calculated by the predict method of the model\n",
    "        Returns a percentage accuracy (float)\n",
    "        \"\"\"\n",
    "\n",
    "        test_set_size = y_predicted.shape[0]\n",
    "        numErrors = np.sum(np.abs(y_predicted - y_test), dtype=float) #output float to force float division\n",
    "        accuracy = 100 - (numErrors/test_set_size)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on wine data\n",
    "X_test = wineData[:, :-1] #delete labels column\n",
    "y_test = wineData[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(data=wineData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.59983704],\n",
       "       [-49.64622986],\n",
       "       [ 22.30777029],\n",
       "       [-19.16805918],\n",
       "       [ -4.4723838 ],\n",
       "       [ 58.64275733],\n",
       "       [-15.13083042],\n",
       "       [-17.15159191],\n",
       "       [-62.12942766],\n",
       "       [ 23.69980479],\n",
       "       [ 71.77616429]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
